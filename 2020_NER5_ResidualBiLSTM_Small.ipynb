{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM + Character-level-embedding + CRF\n",
    "\n",
    "### Running on the small size data to prepare the training dataset\n",
    "\n",
    "https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/\n",
    "\n",
    "https://www.depends-on-the-definition.com/lstm-with-char-embeddings-for-ner/\n",
    "\n",
    "Enhancing sequence tagging: Bidirectional LSTMs With Character Embeddings For Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure Oproject O\n",
      "\n",
      "33139\n",
      "Num words in training set =  25802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob, os, csv, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# AUTO-LABELLED DATA SET = TRAIN SET + VALIDATION SET\n",
    "filename = r\"C:\\Users\\20230326\\1-2 OzROCK\\Files\\autolabelled_smallsize_dictionary_labelled.txt\"\n",
    "\n",
    "#filename = r\"C:\\Users\\20230326\\wamex\\data\\OzROCK_Labeled_Geological_Dataset.txt\"  # 480 files\n",
    "#data = pd.read_csv(filename, header = None, delimiter=\" \", na_values=['\\n'], quoting=csv.QUOTE_NONE, encoding='latin1', skip_blank_lines=True)\n",
    "#print(data.info())\n",
    "#print(data.head(10))\n",
    "\n",
    "#data = dataframes.where((pd.notnull(dataframes)), None)\n",
    "#data = data.fillna('')\n",
    "#data = data.replace(np.nan, '', regex=True)\n",
    "\n",
    "words = []\n",
    "tags = []\n",
    "train_sentences = []\n",
    "\n",
    "with open(filename, 'r') as file :\n",
    "    currSentence = []\n",
    "    lines = []\n",
    "    for line in file:\n",
    "        cols = line.split(\" \")\n",
    "        if line.strip() == '':\n",
    "            # Reset sentence            \n",
    "            train_sentences.append(currSentence)\n",
    "            currSentence = []\n",
    "        elif len(cols) > 2:\n",
    "            print(line)\n",
    "        else:\n",
    "            currSentence.append([cols[0].strip(), cols[1].strip()])\n",
    "            words.append(cols[0].strip())\n",
    "            tags.append(cols[1].strip())\n",
    "print(len(train_sentences)) # 18589; 160,343\n",
    "\n",
    "train_words = list(set(words))\n",
    "print('Num words in training set = ', len(train_words))\n",
    "\n",
    "train_sent_lengths = [len(s) for s in train_sentences]\n",
    "\n",
    "plt.hist(train_sent_lengths, density=1, bins=1000) # x value is bins\n",
    "plt.axis([0, 140, 0, 0.1])  # xmin,xmax,ymin,ymax\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Percentage of sentences')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "Num words in training + test sets =  5582\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST DATA - manually annotated\n",
    "test_filename = r\"C:\\Users\\20230326\\1-2 OzROCK\\Files\\TestSet_March03.txt\" # TestSet_March02\n",
    "filename_result = r\"C:\\Users\\20230326\\1-2 OzROCK\\Files\\TestSet_trueTag_modelTag.txt\"\n",
    "\n",
    "#test_data = pd.read_csv(filename, header = None, delimiter=\" \", na_values=['\\n'], quoting=csv.QUOTE_NONE, encoding='latin1', skip_blank_lines=True)\n",
    "#print(test_data.info())\n",
    "#print(test_data.head(10))\n",
    "words = []\n",
    "test_sentences = []\n",
    "with open(test_filename, 'r') as file :\n",
    "    currSentence = []\n",
    "    lines = []\n",
    "    for line in file:\n",
    "        cols = line.split(\" \")\n",
    "        if line.strip() == '':\n",
    "            # Reset sentence            \n",
    "            test_sentences.append(currSentence)\n",
    "            currSentence = []\n",
    "        elif len(cols) > 2:\n",
    "            print(line)\n",
    "        else :\n",
    "            currSentence.append([cols[0].strip(), cols[1].strip()])            \n",
    "            words.append(cols[0].strip())\n",
    "                \n",
    "print(len(test_sentences)) # 18589; 160,343\n",
    "\n",
    "test_words = list(set(words))\n",
    "print('Num words in training + test sets = ', len(test_words))\n",
    "\n",
    "test_sent_lengths = [len(s) for s in test_sentences]\n",
    "\n",
    "plt.hist(test_sent_lengths, density=1, bins=1000) # x value is bins\n",
    "plt.axis([0, 140, 0, 0.1])  # xmin,xmax,ymin,ymax\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Percentage of sentences')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in all data:  28009\n",
      "\n",
      "Number of unique tags:  13\n",
      "['B-ORE_DEPOSIT', 'B-MINERAL', 'I-ORE_DEPOSIT', 'I-MINERAL', 'B-TIMESCALE', 'B-STRAT', 'B-LOCATION', 'I-STRAT', 'I-LOCATION', 'O', 'I-ROCK', 'B-ROCK', 'I-TIMESCALE']\n",
      "28009\n",
      "['leonardo', 'diemalsr', 'understandably', '21.8', 'background', 'widely', 'encompasses', '86', 'eighty', 'jewell']\n"
     ]
    }
   ],
   "source": [
    "# Word dictionary\n",
    "words = train_words + test_words\n",
    "words = list(set(words))\n",
    "words.append(\"ENDPAD\")\n",
    "\n",
    "n_words = len(words) # unique words\n",
    "print(\"Number of unique words in all data: \", n_words)\n",
    "\n",
    "tags = list(set(tags))\n",
    "n_tags = len(tags)\n",
    "print(\"\\nNumber of unique tags: \", n_tags)\n",
    "print(tags)\n",
    "\n",
    "print(len(words))\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode the character-level information, we will use character embeddings and a LSTM to encode every word to an vector.\n",
    "We can use basically everything that produces a single vector for a sequence of characters that represent a word. You can also use a max-pooling architecture or a CNN or whatever works.\n",
    "Then we feed the vector to another LSTM together with the learned word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14864\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Create dictionaries of words and tags.\n",
    "word2idx = {w: i + 4 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "word2idx[\"null\"] = 2\n",
    "word2idx[\"nan\"] = 3\n",
    "\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "print(word2idx[\"gold\"]) # id=3818\n",
    "print(tag2idx[\"B-ROCK\"]) # id = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(word2idx[\"null\"]) # id=3818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words =  28012\n"
     ]
    }
   ],
   "source": [
    "max_len = 80 # length of each sentence\n",
    "\n",
    "#print(sentences[1])\n",
    "print('Num words = ', len(word2idx))\n",
    "\n",
    "X_word = [[word2idx[w[0]] for w in s] for s in train_sentences]\n",
    "#X_word = []\n",
    "#for s in sentences:\n",
    "#    for w in s:\n",
    "#        try:\n",
    "#            a = [word2idx[w[0]] for w in s]\n",
    "#            X_word.append(a)\n",
    "#        except KeyError as err:\n",
    "#            print('\\nException: ', sys.exc_info()[0], err, '\\n',\n",
    "#                  s, '\\n', w, a)\n",
    "#print(X_word[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Map the senctences to a sequence of numbers and then pad the sequence\n",
    "# we increased the index of the words by one to use zero as a padding value.\n",
    "# This is done because we want to use the mask_zeor parameter of the embedding layer to ignore inputs with value zero\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the sentences\n",
    "X_word = pad_sequences(maxlen=max_len, sequences=X_word, value=word2idx[\"PAD\"], padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leonardo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['leonardo',\n",
       " 'diemalsr',\n",
       " 'understandably',\n",
       " '21.8',\n",
       " 'background',\n",
       " 'widely',\n",
       " 'encompasses',\n",
       " '86',\n",
       " 'eighty',\n",
       " 'jewell']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(words[0])\n",
    "#words[0] = 'NaN'\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_values = []\n",
    "for w in words:\n",
    "    print(w)\n",
    "    if type(w) == str:\n",
    "        word_values.append(w)\n",
    "\n",
    "print(len(word_values))\n",
    "chars = set([w_i for w in word_values for w_i in w])\n",
    "n_chars = len(chars)\n",
    "print(n_chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Generate a dictionary for the characters we want to use and create the sequence of characters for every token,\n",
    "# set to 10.\n",
    "# We could also use longer or shorter sequences. \n",
    "# We could even use two sequences, one with the five first characters and one with the five last chars\n",
    "max_len_char = 30 # character length\n",
    "print(max_len_char)\n",
    "\n",
    "# tags = list(set(data[\"Tag\"].values))\n",
    "#chars = set([w_i for w in words for w_i in w])\n",
    "#n_chars = len(chars)\n",
    "#print(n_chars)\n",
    "#print(chars)\n",
    "\n",
    "import string\n",
    "chars = [s for s in string.printable]\n",
    "n_chars = len(chars)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char2idx[\"PAD\"] = 0\n",
    "char2idx[\"UNK\"] = 1\n",
    "len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char = []\n",
    "for sentence in train_sentences:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"PAD\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char.append(np.array(sent_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 7, 10, 10, 10, 10, 10, 10, 12, 11, 11, 10, 10, 12, 11, 10, 10, 10, 12, 11, 10]\n",
      "[10  7 10 10 10 10 10 10 12 11 11 10 10 12 11 10 10 10 12 11 10  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# Map tags to a sequence numbers and the pad\n",
    "y = [[tag2idx[w[1]] for w in s] for s in train_sentences]\n",
    "print(y[0])\n",
    "\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, value=tag2idx[\"PAD\"], padding='post', truncating='post')\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(X_char[5]))\n",
    "print(X_char[5].shape)\n",
    "print(X_char[5][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split in train and test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_word_tr, X_word_te, y_tr, y_te = train_test_split(X_word, y, test_size=0.2, random_state=45)\n",
    "X_char_tr, X_char_te, _, _ = train_test_split(X_char, y, test_size=0.2, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26511, 80)\n",
      "(6628, 80)\n",
      "26511\n",
      "6628\n",
      "26511\n",
      "6628\n"
     ]
    }
   ],
   "source": [
    "print(X_word_tr.shape)\n",
    "print(X_word_te.shape)\n",
    "\n",
    "print(len(X_char_tr))\n",
    "print(len(X_char_te))\n",
    "\n",
    "print(len(y_tr))\n",
    "print(len(y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the character embedding model\n",
    "Trick is to wrap the parts that should be applied to the characters in a TimeDistributed layer to apply the same layers to every character sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# input and embedding for words\n",
    "word_in = Input(shape=(max_len,))\n",
    "emb_word = Embedding(input_dim=n_words + 4, output_dim=64, input_length=max_len, mask_zero=True)(word_in)\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(max_len, max_len_char,))\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=30, input_length=max_len_char, mask_zero=True))(char_in)\n",
    "\n",
    "# character LSTM to get word encodings by characters\n",
    "char_enc = TimeDistributed(LSTM(units=100, return_sequences=False, recurrent_dropout=0.5))(emb_char)\n",
    "\n",
    "# main LSTM\n",
    "x = concatenate([emb_word, char_enc])\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "\n",
    "main_lstm = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.5))(x)\n",
    "\n",
    "main_dense = TimeDistributed(Dense(100, activation=\"relu\"))(main_lstm)  # a dense layer as suggested by neuralNer\n",
    "\n",
    "out = TimeDistributed(Dense(n_tags + 1, activation=\"softmax\"))(main_dense)\n",
    "\n",
    "model = Model([word_in, char_in], out)\n",
    "\n",
    "# Train the model: compile the model and look at the summary.\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 16:01:07.425378  6876 deprecation.py:323] From C:\\Users\\20230326\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1850a665c466>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# input and embedding for words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mword_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0memb_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_words\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[0;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                              input_tensor=tensor)\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'input'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[1;34m(prefix)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#import tensorflow_hub as hub\n",
    "#from keras import backend as K\n",
    "from tensorflow import keras\n",
    "#from keras.models import Model\n",
    "#from keras.layers import Input\n",
    "#from keras.layers.merge import add\n",
    "#from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# input and embedding for words\n",
    "word_in = Input(shape=(max_len,))\n",
    "emb_word = Embedding(input_dim=n_words + 4, output_dim=64, input_length=max_len, mask_zero=True)(word_in)\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(max_len, max_len_char,))\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=30, input_length=max_len_char, mask_zero=True))(char_in)\n",
    "\n",
    "# character LSTM to get word encodings by characters\n",
    "char_enc = TimeDistributed(LSTM(units=100, return_sequences=False, recurrent_dropout=0.5))(emb_char)\n",
    "\n",
    "# main LSTM\n",
    "x = concatenate([emb_word, char_enc])\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "\n",
    "main_lstm = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.5))(x)\n",
    "\n",
    "main_dense = TimeDistributed(Dense(100, activation=\"relu\"))(main_lstm)  # a dense layer as suggested by neuralNer\n",
    "\n",
    "out = TimeDistributed(Dense(n_tags + 1, activation=\"softmax\"))(main_dense)\n",
    "\n",
    "model = Model([word_in, char_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_text = Input(shape=(max_len,), dtype=tf.string)\n",
    "embedding = Lambda(ElmoEmbedding, output_shape=(None, 1024))(input_text)\n",
    "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
    "\n",
    "model = Model(input_text, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "plot_model(model, to_file='model2_WL_CL_BiLSTM.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit([X_word_tr,\n",
    "#                     np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))],\n",
    "#                    np.array(y_tr).reshape(len(y_tr), max_len, 1),\n",
    "#                    batch_size=32, epochs=100, validation_split=0.1, verbose=1)\n",
    "history = model.fit([X_word_tr,\n",
    "                     np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))],\n",
    "                    np.array(y_tr).reshape(len(y_tr), max_len, 1),\n",
    "                    batch_size=32, epochs=10, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(hist[\"loss\"])\n",
    "plt.plot(hist[\"val_loss\"])\n",
    "\n",
    "#plt.figure(figsize=(5,5))\n",
    "#plt.plot(hist[\"acc\"])\n",
    "#plt.plot(hist[\"val_acc\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save(r\"C:/Users/20230326/wamex/data/2020_model_WL_CL_BiLSTM_Small.h5\")  # creates a HDF5 file 'my_model.h5'\n",
    "#del model  # deletes the existing model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TEST this\n",
    "# returns a compiled model, identical to the previous one\n",
    "model_loaded = load_model(r\"C:/Users/20230326/wamex/data/2020_model_WL_CL_BiLSTM_Small.h5\")\n",
    "y_pred = model_loaded.predict([X_word_te,\n",
    "                        np.array(X_char_te).reshape((len(X_char_te), max_len, max_len_char))])\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X_word_te,\n",
    "                        np.array(X_char_te).reshape((len(X_char_te), max_len, max_len_char))])\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(i):\n",
    "    p = np.argmax(y_pred[i], axis=-1)\n",
    "    print(\"{:15}|{:15}|{}\".format(\"Word\", \"Truth\", \"Predicted\"))\n",
    "    print(40 * \"=\")\n",
    "\n",
    "    for w, t, pred in zip(X_word_te[i], y_te[i], p):\n",
    "        if w != 0:\n",
    "            print(\"{:15} {:15} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#for i in range(0, len(y_pred)):\n",
    "for i in range(0, 10):\n",
    "    print_prediction(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred) # Number of sentences in prediction / test set\n",
    "\n",
    "def get_results(i):\n",
    "    words_value = []\n",
    "    tags_true = []\n",
    "    tags_pred = []\n",
    "    p = np.argmax(y_pred[i], axis=-1)\n",
    "    #print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "    #print(30 * \"=\")\n",
    "\n",
    "    for w, t, pred in zip(X_word_te[i], y_te[i], p):\n",
    "        if w != 0:\n",
    "            #print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))\n",
    "            words_value.append(idx2word[w])\n",
    "            tags_true.append(idx2tag[t])\n",
    "            tags_pred.append(idx2tag[pred])\n",
    "            \n",
    "    return words_value, tags_true, tags_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0, len(y_pred)):\n",
    "words_tagged = []\n",
    "labels_true = []\n",
    "labels_predicted = []\n",
    "\n",
    "for i in range(0, len(y_pred)):\n",
    "    words_value, tags_true, tags_pred = get_results(i)\n",
    "    words_tagged.extend(words_value)\n",
    "    labels_true.extend(tags_true)\n",
    "    labels_predicted.extend(tags_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "print(len(labels_true))\n",
    "#print(y_te[0])\n",
    "\n",
    "print(len(labels_predicted))\n",
    "#print(y_pred[0])\n",
    "\n",
    "#print(classification_report(labels_true, labels_predicted))\n",
    "print(classification_report(labels_true, labels_predicted, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_true[:20])\n",
    "print(labels_predicted[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SET\n",
    "## Manually annotated dataset for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word ids for each sentence\n",
    "test_X_word = [[word2idx[w[0]] for w in s] for s in test_sentences]\n",
    "\n",
    "# Pad the sentences\n",
    "test_X_word = pad_sequences(maxlen=max_len, sequences=test_X_word, value=word2idx[\"PAD\"], padding='post', truncating='post')\n",
    "\n",
    "test_X_char = []\n",
    "for sentence in test_sentences:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"PAD\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    test_X_char.append(np.array(sent_seq))\n",
    "    \n",
    "# Map tags to a sequence numbers and the pad\n",
    "test_y = [[tag2idx[w[1]] for w in s] for s in test_sentences]\n",
    "print(y[0])\n",
    "\n",
    "test_y = pad_sequences(maxlen=max_len, sequences=test_y, value=tag2idx[\"PAD\"], padding='post', truncating='post')\n",
    "print(test_y[0])\n",
    "\n",
    "print('Test set word shape = ', test_X_word.shape)\n",
    "print('Test set character length = ', len(test_X_char))\n",
    "print('Test set labels length = ', len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels on the manually created TEST SET\n",
    "test_y_pred = model.predict([test_X_word, np.array(test_X_char).reshape((len(test_X_char), max_len, max_len_char))])\n",
    "len(test_y_pred) # Number of sentences in prediction / test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_results(i):\n",
    "    words_value = []\n",
    "    tags_true = []\n",
    "    tags_pred = []\n",
    "    p = np.argmax(test_y_pred[i], axis=-1)\n",
    "    #print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "    #print(30 * \"=\")\n",
    "\n",
    "    for w, t, pred in zip(test_X_word[i], test_y[i], p):\n",
    "        if w != 0:\n",
    "            #print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))\n",
    "            words_value.append(idx2word[w])\n",
    "            tags_true.append(idx2tag[t])\n",
    "            tags_pred.append(idx2tag[pred])\n",
    "            \n",
    "    return words_value, tags_true, tags_pred\n",
    "\n",
    "words_tagged = []\n",
    "labels_true = []\n",
    "labels_predicted = []\n",
    "\n",
    "for i in range(0, len(test_y_pred)):\n",
    "    words_value, tags_true, tags_pred = get_test_results(i)\n",
    "    words_tagged.extend(words_value)\n",
    "    labels_true.extend(tags_true)\n",
    "    labels_predicted.extend(tags_pred)\n",
    "    \n",
    "print(len(labels_true))\n",
    "#print(y_te[0])\n",
    "\n",
    "print(len(labels_predicted))\n",
    "#print(y_pred[0])\n",
    "\n",
    "#print(classification_report(labels_true, labels_predicted))\n",
    "print(classification_report(labels_true, labels_predicted, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(filename_result, 'w')\n",
    "f.write(\"Word, GroundTruth, Predicted\\n\")\n",
    "for i in range(0, len(test_y_pred)):    \n",
    "    p = np.argmax(test_y_pred[i], axis=-1)    \n",
    "    #print(40 * \"=\")\n",
    "\n",
    "    for w, t, pred in zip(test_X_word[i], test_y[i], p):\n",
    "        if w != 0:\n",
    "            #print(\"{:15} {:15} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))  \n",
    "            f.write(idx2word[w] + ',' + idx2tag[t] + ',' + idx2tag[pred] + '\\n')\n",
    "f.close()\n",
    "#for i in range(0, 10):\n",
    "    #test_prediction(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(i):    \n",
    "    p = np.argmax(test_y_pred[i], axis=-1)\n",
    "    print(\"{:15}|{:15}|{}\".format(\"Word\", \"GroundTruth\", \"Predicted\"))\n",
    "    print(40 * \"=\")\n",
    "\n",
    "    for w, t, pred in zip(test_X_word[i], test_y[i], p):\n",
    "        if w != 0:\n",
    "            print(\"{:15} {:15} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c = 0\n",
    "for s in test_sentences:\n",
    "    for w in s:\n",
    "        c += 1\n",
    "        print(c, w, tag2idx[w[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#word2idx.keys()\n",
    "word2idx.values()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_char = []\n",
    "sentence = [‘Geographical’]\n",
    "max_len = 3\n",
    "max_len_char = 20\n",
    "\n",
    "for i in range(max_len):\n",
    "word_seq = []\n",
    "for j in range(max_len_char):\n",
    "try:\n",
    "word_seq.append(char2idx.get(sentence[i][j]))\n",
    "except:\n",
    "word_seq.append(char2idx.get(“PAD”))\n",
    "X_char.append(word_seq)\n",
    "\n",
    "X_char = [np.array(X_char)]\n",
    "X_word = [[word2idx[s] if s in word2idx.keys() else word2idx[‘UNK’] for s in sentence]]\n",
    "X_word = pad_sequences(maxlen=max_len, sequences=X_word, value=word2idx[“PAD”], padding=’post’, truncating=’post’)\n",
    "X_word = np.array(X_word)\n",
    "y_pred = model.predict([X_word, np.array(X_char).reshape((len(X_char), max_len, max_len_char))])\n",
    "p = np.argmax(y_pred[0], axis=-1)\n",
    "\n",
    "for pred in p:\n",
    "print(idx2tag[pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
