{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ner.csv']\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "# Source: https://colab.research.google.com/drive/1siYfyA34i4ZA4LdnOf9P_hR_THkJd6MA#scrollTo=9QaJKZHXZbj6\n",
    "    \n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "\n",
    "# !pip install transformers\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"input/\"))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050795, 25)\n",
      "   Unnamed: 0     lemma next-lemma next-next-lemma next-next-pos  \\\n",
      "0           0  thousand         of        demonstr           NNS   \n",
      "1           1        of   demonstr            have           VBP   \n",
      "2           2  demonstr       have           march           VBN   \n",
      "3           3      have      march         through            IN   \n",
      "4           4     march    through          london           NNP   \n",
      "\n",
      "  next-next-shape next-next-word next-pos next-shape      next-word  ...  \\\n",
      "0       lowercase  demonstrators       IN  lowercase             of  ...   \n",
      "1       lowercase           have      NNS  lowercase  demonstrators  ...   \n",
      "2       lowercase        marched      VBP  lowercase           have  ...   \n",
      "3       lowercase        through      VBN  lowercase        marched  ...   \n",
      "4     capitalized         London       IN  lowercase        through  ...   \n",
      "\n",
      "  prev-prev-lemma prev-prev-pos prev-prev-shape prev-prev-word   prev-shape  \\\n",
      "0      __start2__    __START2__        wildcard     __START2__     wildcard   \n",
      "1      __start1__    __START1__        wildcard     __START1__  capitalized   \n",
      "2        thousand           NNS     capitalized      Thousands    lowercase   \n",
      "3              of            IN       lowercase             of    lowercase   \n",
      "4        demonstr           NNS       lowercase  demonstrators    lowercase   \n",
      "\n",
      "       prev-word sentence_idx        shape           word tag  \n",
      "0     __START1__          1.0  capitalized      Thousands   O  \n",
      "1      Thousands          1.0    lowercase             of   O  \n",
      "2             of          1.0    lowercase  demonstrators   O  \n",
      "3  demonstrators          1.0    lowercase           have   O  \n",
      "4           have          1.0    lowercase        marched   O  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "#dframe = pd.read_csv(\"input/ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)\n",
    "df_data = pd.read_csv(\"input/ner.csv\",sep=\",\",encoding=\"latin1\", error_bad_lines=False).fillna(method='ffill')\n",
    "print(df_data.shape)\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "tag_list=df_data.tag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "cb9e29cd3d9de99b21ebfedb930c4d59e813664d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((840636, 25), (210159, 25))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test=train_test_split(df_data,test_size=0.20,shuffle=False)\n",
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "c598cf2c955b71ac3371bf6ebb19169b5734e5f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VBP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VBN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos  sentence_idx           word tag\n",
       "0  NNS           1.0      Thousands   O\n",
       "1   IN           1.0             of   O\n",
       "2  NNS           1.0  demonstrators   O\n",
       "3  VBP           1.0           have   O\n",
       "4  VBN           1.0        marched   O"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=dframe.drop(['Unnamed: 0', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n",
    "       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n",
    "       'next-word', 'prev-iob', 'prev-lemma', 'prev-pos',\n",
    "       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n",
    "       'prev-prev-word', 'prev-shape', 'prev-word','shape'],axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "c31cfa31598d78e73db241816ee8f7d98e7b320f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25549 9629\n"
     ]
    }
   ],
   "source": [
    "agg_func = lambda s: [ [w,t] for w,t in zip(s[\"word\"].values.tolist(),s[\"tag\"].values.tolist())]\n",
    "\n",
    "x_train_grouped = x_train.groupby(\"sentence_idx\").apply(agg_func)\n",
    "x_test_grouped = x_test.groupby(\"sentence_idx\").apply(agg_func)\n",
    "print(len(x_train_grouped), len(x_test_grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25549 9629\n",
      "['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.', 'Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.'] \n",
      " ['1609', ',', 'but', 'they', 'remained', 'uninhabited', 'until', 'the', '19th', 'century', '.']\n"
     ]
    }
   ],
   "source": [
    "x_train_sentences = [[s[0] for s in sent] for sent in x_train_grouped.values]\n",
    "x_test_sentences = [[s[0] for s in sent] for sent in x_test_grouped.values]\n",
    "print(len(x_train_sentences), len(x_test_sentences))\n",
    "print(x_train_sentences[0], '\\n', x_test_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25549 9629\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O'] \n",
      " ['B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25549,), (9629,))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tags = [[t[1] for t in tag] for tag in x_train_grouped.values]\n",
    "x_test_tags = [[t[1] for t in tag] for tag in x_test_grouped.values]\n",
    "print(len(x_train_tags), len(x_test_tags))\n",
    "print(x_train_tags[0], '\\n', x_test_tags[0])\n",
    "np.shape(x_train_tags),np.shape(x_test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 12:37:57.768949 15008 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "C:\\Users\\20230326\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\20230326\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\20230326\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\20230326\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\20230326\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\20230326\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TFBertForTokenClassification' from 'transformers' (C:\\Users\\20230326\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-116a42db55b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msequence_a_segment_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m from transformers import (\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mTF2_WEIGHTS_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mBertConfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TFBertForTokenClassification' from 'transformers' (C:\\Users\\20230326\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH=128\n",
    "BERT_MODEL=\"bert-base-multilingual-cased\"\n",
    "\n",
    "BATCH_SIZE=32\n",
    "\n",
    "pad_token=0,\n",
    "pad_token_segment_id=0,\n",
    "sequence_a_segment_id=0,\n",
    "\n",
    "from transformers import (\n",
    "    TF2_WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertForTokenClassification,\n",
    "    create_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\"bert\": (BertConfig, TFBertForTokenClassification, BertTokenizer)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(tag_list)}\n",
    "num_labels = len(tag_list) + 1\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_label_id = 0\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "config = config_class.from_pretrained(BERT_MODEL,num_labels=num_labels)\n",
    "tokenizer = tokenizer_class.from_pretrained(BERT_MODEL,do_lower_case=False)\n",
    "\n",
    "model = model_class.from_pretrained(\n",
    "                BERT_MODEL,\n",
    "                from_pt=bool(\".bin\" in BERT_MODEL),\n",
    "                config=config)\n",
    "\n",
    "model.layers[-1].activation = tf.keras.activations.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_seq_length =128\n",
    "\n",
    "def convert_to_input(sentences,tags):\n",
    "  input_id_list,attention_mask_list,token_type_id_list=[],[],[]\n",
    "  label_id_list=[]\n",
    "  \n",
    "  for x,y in tqdm(zip(sentences,tags),total=len(tags)):\n",
    "  \n",
    "    tokens = []\n",
    "    label_ids = []\n",
    "\n",
    "    for word, label in zip(x, y):\n",
    "      word_tokens = tokenizer.tokenize(word)\n",
    "      tokens.extend(word_tokens)\n",
    "      # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "      label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "  \n",
    "    special_tokens_count =  2\n",
    "    if len(tokens) > max_seq_length - special_tokens_count:\n",
    "      tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "      label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "    label_ids = [pad_token_label_id]+label_ids+[pad_token_label_id]\n",
    "    inputs = tokenizer.encode_plus(tokens,add_special_tokens=True, max_length=max_seq_length)\n",
    "\n",
    "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "    attention_masks = [1] * len(input_ids)\n",
    "\n",
    "    attention_mask_list.append(attention_masks)\n",
    "    input_id_list.append(input_ids)\n",
    "    token_type_id_list.append(token_type_ids)\n",
    "\n",
    "    label_id_list.append(label_ids)\n",
    "\n",
    "  return input_id_list,token_type_id_list,attention_mask_list,label_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train,token_ids_train,attention_masks_train,label_ids_train=convert_to_input(x_train_sentences,x_train_tags)\n",
    "input_ids_test,token_ids_test,attention_masks_test,label_ids_test=convert_to_input(x_test_sentences,x_test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = pad_sequences(input_ids_train,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "token_ids_train = pad_sequences(token_ids_train,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "attention_masks_train = pad_sequences(attention_masks_train,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "label_ids_train = pad_sequences(label_ids_train,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "\n",
    "np.shape(input_ids_train),np.shape(token_ids_train),np.shape(attention_masks_train),np.shape(label_ids_train),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = pad_sequences(input_ids_test,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "token_ids_test = pad_sequences(token_ids_test,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "attention_masks_test = pad_sequences(attention_masks_test,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "label_ids_test = pad_sequences(label_ids_test,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "\n",
    "np.shape(input_ids_test),np.shape(token_ids_test),np.shape(attention_masks_test),np.shape(label_ids_test),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_to_features(input_ids,attention_masks,token_type_ids,y):\n",
    "  return {\"input_ids\": input_ids,\n",
    "          \"attention_mask\": attention_masks,\n",
    "          \"token_type_ids\": token_type_ids},y\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((input_ids_train,attention_masks_train,token_ids_train,label_ids_train)).map(example_to_features).shuffle(1000).batch(32).repeat(5)\n",
    "\n",
    "\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((input_ids_test,attention_masks_test,token_ids_test,label_ids_test)).map(example_to_features).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in test_ds.take(10):\n",
    "  print(x,y)\n",
    "\n",
    "model.summary()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "history = model.fit(train_ds, epochs=3, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
