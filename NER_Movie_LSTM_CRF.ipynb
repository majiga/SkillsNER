{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie dataset NER\n",
    "Source: https://aihub.cloud.google.com/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba\n",
    "\n",
    "The notebook is broken down into the following three sections:\n",
    "   * NER packages: An overview of the language, license, and methodology of commercially available NER packages\n",
    "   * NER with SpaCy : Code examples for training and serving a custom NER model in SpaCy\n",
    "   * NER with Tensorflow : Code examples for creating, training, and serving a custom deep-learning NER model with Tensorflow\n",
    "\n",
    "NER may be implemented with a variety of statistical and rule-based methods with varying amounts of feature engineering. All production-ready NER methods are at least semi-supervised, though unsupervised approaches are an emerging research topic.\n",
    "\n",
    "#### Supervised statistical\n",
    "\n",
    "Supervised statistical approaches to NER typically use either Hidden Markov Models (HMM), Maximum Entropy (ME), or Conditional Random Fields (CRF). OpenNLP's statistical NER relies on ME. GATE relies on HMM.\n",
    "Typical feature engineering approaches for NER include such approaches as orthography, n-grams, lexicons, suffixes and prefixes, unsupervised cluster features, and trigger words for named entities (such as river or lake). These features are generated algorithmically in a rule-based manner.\n",
    "    \n",
    "#### Supervised rule-based\n",
    "\n",
    "OpenNLP contains rule based (as well as statistical) NER. The rule-based approach relies on a series of regular expression matches. The feature generation seems to be done with a beam search to determine the word context.\n",
    "DBPedia spotlight performs NER with substring matching using the Aho-Corasick algorithm. The approach only uses tokenization with no other feature engineering. The two-step approach first involves generating all possible candidate annotations that form known labels. This is rule-based in that it involves identifying nouns, prepositions, capitalized words, and known entities. This is based on OpenNLP under the hood. The second step selects the best candidates from the proposed candidates. Each candidate is scored based on annotation probability using a version of tf-idf with article links and anchor texts instead of documents and terms.\n",
    "\n",
    "#### Supervised deep learning\n",
    "\n",
    "**[SpaCy](https://spacy.io),** which is one of the most popular productionized NER environments, **uses residual convolutional neural networks (CNN) and incremental parsing with Bloom embeddings for NER.** See [this](https://www.youtube.com/watch?v=sqDHBH9IjRU) Youtube explanation from the developers for more detail. To summarize the algorithm, 1D convolutional filters are applied over the input text to predict how the upcoming words may change the current entity tags. Upcoming words may either shift (change the entity), reduce (make the entity more granular), or output the entity. The input sequence is embedded with bloom embeddings, which model the characters, prefix, suffix, and part of speech of each word. Residual blocks are used for the CNNs, andn the filter sizes are chosen with beam search.\n",
    "\n",
    "Recurrent neural network (RNN) approaches to NER also exist, typically comprising long short term memory networks (LSTM) at either the word- or character-level, relying on word or character embeddings, respectively (e.g. word2vec, gloVe, FASTtext).\n",
    "    \n",
    "## NER Evaluation metrics\n",
    "\n",
    "NER is most commonly evaluated with precision, recall, and F1-score. F1-score can either be relaxed or strict, with the latter requiring the character offsets to match exactly. \n",
    "\n",
    "## Movie NER\n",
    "While these pretrained models are often sufficient for general applications, we will consider a domain-specific application of NER on the [MIT Movies corpus](https://groups.csail.mit.edu/sls/downloads/movie/), which contains 10,000 queries about various aspects of movies, with the following entity labels:\n",
    "\n",
    "| Type | Example |\n",
    "------- | ------- |\n",
    "| ACTOR | Matt Damon |\n",
    "| YEAR | 1980s |\n",
    "| TITLE | Pulp Fiction\n",
    "| GENRE | science fiction\n",
    "| DIRECTOR | George Lucas |\n",
    "| SONG | Aerosmith |\n",
    "| PLOT | Flying cars |\n",
    "| REVIEW | must see |\n",
    "| CHARACTER | Queen Elizabeth |\n",
    "|RATING | PG-13 |\n",
    "|RATINGS_AVERAGE | best rated |\n",
    "| TRAILER | preview\n",
    "\n",
    "As these tables show, the pretrained SpaCy models would not be sufficient to identify entities to help answer a question such as \"did george clooney make a science fiction movie in the 1980s?\" While the pre-trained entities may identify the presence of `PERSON`, `DATE`, and `PRODUCT`, a custom model should be able to detect `ACTOR`, `GENRE`, and `DATE`. In the following sections, we will compare the results of applying a pre-trained and a custom-trained model to the MIT movies corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (2.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.47.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/majiga/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy # !{sys.executable} ensures package installation in conda env\n",
    "\n",
    "import random\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, mkdir\n",
    "if not path.isdir(\"data/\"):\n",
    "    mkdir(\"data/\")\n",
    "if not path.isdir(\"models/\"):\n",
    "    mkdir(\"models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the test and training dataset from MIT's Computer Science and Aritficial Intelligence Laboratory (CSAIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  246k  100  246k    0     0  90713      0  0:00:02  0:00:02 --:--:-- 90680\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  989k  100  989k    0     0   162k      0  0:00:06  0:00:06 --:--:--  213k\n"
     ]
    }
   ],
   "source": [
    "!curl https://groups.csail.mit.edu/sls/downloads/movie/engtest.bio -o data/test.txt\n",
    "!curl https://groups.csail.mit.edu/sls/downloads/movie/engtrain.bio -o data/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.3/kfp-components/notebooks/entity_extraction/assets/fig1.png\" width=\"700\" align = \"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy requires training data to be in the format of `TRAIN_DATA = [(Sentence, {entities: [(start, end, label)]}, ...]`. The `load_data` function parses and transforms the input data into the required format for spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_spacy(file_path):\n",
    "    ''' Converts data from:\n",
    "    label \\t word \\n label \\t word \\n \\n label \\t word\n",
    "    to: sentence, {entities : [(start, end, label), (stard, end, label)]}\n",
    "    '''\n",
    "    file = open(file_path, 'r')\n",
    "    training_data, entities, sentence, unique_labels = [], [], [], []\n",
    "    current_annotation = None\n",
    "    end = 0 # initialize counter to keep track of start and end characters\n",
    "    for line in file:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        # lines with len > 1 are words\n",
    "        if len(line) > 1:\n",
    "            label = line[0][2:]     # the .txt is formatted: label \\t word, label[0:2] = label_type\n",
    "            label_type = line[0][0] # beginning of annotations - \"B\", intermediate - \"I\"\n",
    "            word = line[1]\n",
    "            sentence.append(word)\n",
    "            end += (len(word) + 1)  # length of the word + trailing space\n",
    "            \n",
    "            if label_type != 'I' and current_annotation:  # if at the end of an annotation\n",
    "                entities.append((start, end - 2 - len(word), current_annotation))  # append the annotation\n",
    "                current_annotation = None                 # reset the annotation\n",
    "            if label_type == 'B':                         # if beginning new annotation\n",
    "                start = end - len(word) - 1  # start annotation at beginning of word\n",
    "                current_annotation = label   # append the word to the current annotation\n",
    "            if label_type == 'I':            # if the annotation is multi-word\n",
    "                current_annotation = label   # append the word\n",
    "            \n",
    "            if label != 'O' and label not in unique_labels:\n",
    "                unique_labels.append(label)\n",
    " \n",
    "        # lines with len == 1 are breaks between sentences\n",
    "        if len(line) == 1: \n",
    "            if current_annotation:\n",
    "                entities.append((start, end - 1, current_annotation))\n",
    "            sentence = \" \".join(sentence)\n",
    "            training_data.append([sentence, {'entities' : entities}])\n",
    "            # reset the counters and temporary lists\n",
    "            end = 0            \n",
    "            entities, sentence = [], []\n",
    "            current_annotation = None\n",
    "    file.close()\n",
    "    return training_data, unique_labels            \n",
    "            \n",
    "TRAIN_DATA, LABELS = load_data_spacy(\"data/train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample sentences from the training data, which contains queries about movie information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['show me films with drew barrymore from the 1980s',\n",
       " 'what movies starred both al pacino and robert deniro',\n",
       " 'find me all of the movies that starred harold ramis and bill murray',\n",
       " 'find me a movie with a quote about baseball in it',\n",
       " 'what movies have mississippi in the title',\n",
       " 'show me science fiction films directed by steven spielberg',\n",
       " 'do you have any thrillers directed by sofia coppola',\n",
       " 'what leonard cohen songs have been used in a movie',\n",
       " 'show me films elvis films set in hawaii']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in TRAIN_DATA[1:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample labeled annotations for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entities': [(19, 33, 'ACTOR'), (43, 48, 'YEAR')]},\n",
       " {'entities': [(25, 34, 'ACTOR'), (39, 52, 'ACTOR')]},\n",
       " {'entities': [(39, 51, 'ACTOR'), (56, 67, 'ACTOR')]},\n",
       " {'entities': []},\n",
       " {'entities': [(17, 28, 'TITLE')]},\n",
       " {'entities': [(8, 29, 'GENRE'), (42, 58, 'DIRECTOR')]},\n",
       " {'entities': [(16, 25, 'GENRE'), (38, 51, 'DIRECTOR')]},\n",
       " {'entities': [(5, 24, 'SONG')]},\n",
       " {'entities': [(14, 19, 'ACTOR'), (26, 39, 'PLOT')]}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[1] for x in TRAIN_DATA[1:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pre-trained NER Model\n",
    "\n",
    "First, download the pre-trained model with a subprocess call."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained model fails to identify any genres, plots, actors, directors, characters, movie titles, or ratings present in the movie queries. Interestingly, it also fails to identify persons, works of art, and products! Clearly, the pretrained model does not fit this domain application, so we will train our own model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "Model performance is assessed on the entirety of the test dataset (2,443 sentences) based on the following metrics and their definitions.\n",
    "\n",
    "   * Precision: true positives / (true positives + false positives)\n",
    "   * Recall: true positives / (true positives + false negatives)\n",
    "   * F1-score: harmonic average of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(pred, true):        \n",
    "    precision = len([x for x in pred if x in true]) / (len(pred) + 1e-20) # true positives / total pred\n",
    "    return precision\n",
    "\n",
    "def calc_recall(pred, true):\n",
    "    recall = len([x for x in true if x in pred]) / (len(true) + 1e-20)    # true positives / total test\n",
    "    return recall\n",
    "\n",
    "def calc_f1(precision, recall):\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + 1e-20))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from itertools import chain\n",
    "\n",
    "# run the predictions on each sentence in the test dataset, and return the spacy object\n",
    "preds = [ner(x[0]) for x in TEST_DATA]\n",
    "\n",
    "precisions, recalls, f1s = [], [], [] \n",
    "\n",
    "# iterate over predictions and test data and calculate precision, recall, and F1-score\n",
    "for pred, true in zip(preds, TEST_DATA): \n",
    "    true = [x[2] for x in list(chain.from_iterable(true[1].values()))] # x[2] = annotation, true[1] = (start, end, annot)\n",
    "    pred = [i.label_ for i in pred.ents] # i.label_ = annotation label, pred.ents = list of annotations\n",
    "    precision = calc_precision(true, pred)\n",
    "    precisions.append(precision)\n",
    "    recall = calc_recall(true, pred)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(calc_f1(precision, recall))\n",
    "    \n",
    "print(\"Precision: {} \\nRecall: {} \\nF1-score: {}\".format(np.around(np.mean(precisions), 3),\n",
    "                                                         np.around(np.mean(recalls), 3),\n",
    "                                                         np.around(np.mean(f1s), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.3/kfp-components/notebooks/entity_extraction/assets/fig7.png\" align = \"left\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named entity recognition with Tensorflow\n",
    "\n",
    "This section focuses on developing, training, and serving a custom NER architecture with Tensorflow 1.9.0. We will implement an LSTM-CRF model as described in [Huang, Xu, and Yu, 2015](https://arxiv.org/pdf/1508.01991.pdf).\n",
    "\n",
    "This approach can be broken down into its constituent parts as follows:\n",
    "   * Embedding: Generating a dense vector representation of words\n",
    "   * LSTM: Incorporating past and future features to generate a representation of each time step\n",
    "   * CRF: Make use of neighboring information to predict current tags. The CRF approach has been shown to provide higher accuracy than maximum entropy models because CRF considers the entire sentence rather than relying on beam search to find optimal context sizes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from functools import partial\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "params = {\n",
    "    'dim' : 300,            # dimension of embeddings\n",
    "    'maximum_steps' : 1000, # number of training steps        \n",
    "    'lstm_size' : 150,      # dimension of LSTM\n",
    "    'batch_size' : 25,      # batch size\n",
    "    'max_words' : 10000,    # maximum number of words to embed\n",
    "    'padding_size' : 20,    # maximum sentence size\n",
    "    'num_classes' : 14,     # number of unique classes\n",
    "    'save_dir' : 'models/' # directory to save hash tables, model weights, etc.\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!pip install -U pip keras tensorflow\n",
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in implementing a tensorflow named entity recognition architecture is to specify the data loading and transformation process. The words and labels need to be transformed to an integer vector format that tensorflow can process. Tokenization is used to do this, where unique words and labels are mapped to integers and the mapping is stored in a hashtable for back-conversion. \n",
    "\n",
    "For this process to work, however, we have to see all of the training data all at once to prevent overlapping hashes. This means that this tokenization process needs to happen separately from the training process. The `make_tokenizer` function takes in the training data and labels and returns two dictionaries, `word_index`, and `labels_index`. The former specifies integer mappings for the words, and the latter specifies integer mappings for the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(directory, obj, name):\n",
    "    '''Helper function using pickle to save and load objects'''\n",
    "    with open(directory + name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(directory, name):\n",
    "    '''Helper function using pickle to save and load objects'''\n",
    "    with open(directory + name + \".pkl\", \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def load_data(file = \"data/train.txt\"):\n",
    "    '''Helper function to load and transform inputs and labels\n",
    "    included as a separate function due to NER-specific evaluation needs:\n",
    "        tensorflow does not have multi-class precision/accuracy as a metric\n",
    "        so data_y is needed to manually calculate evaluations'''\n",
    "    file = open(file, 'r')\n",
    "    sentence, labels = [], []\n",
    "    data_x, data_y = [], []\n",
    "    for line in file:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        # lines with len > 1 are words\n",
    "        if len(line) > 1:\n",
    "            sentence.append(line[1])\n",
    "            labels.append(line[0][2:]) if len(line[0]) > 1 else labels.append(line[0])\n",
    "        \n",
    "        # lins with len == 1 are sentence breaks\n",
    "        if len(line) == 1: \n",
    "            data_x.append(' '.join(sentence))\n",
    "            data_y.append(labels)\n",
    "            sentence, labels = [], []\n",
    "    return data_x, data_y\n",
    "\n",
    "def make_tokenizer(file = \"data/train.txt\", params = params):\n",
    "    ''' In order for one hot encoding of words and labels to work, \n",
    "    every word and label has to be seen at least once to make a hashing table.\n",
    "    This function outputs hash tables for the words and the labels\n",
    "    that can be used to one-hot-encode them in the generator\n",
    "    '''\n",
    "    # Load parameters and data\n",
    "    max_words = params['max_words']\n",
    "    padding_size = params['padding_size']\n",
    "    save_dir = params['save_dir']\n",
    "    data_x, data_y = load_data(file)\n",
    "            \n",
    "    # Use the Keras tokenizer API to generate hashing table for data_x\n",
    "    tokenizer = Tokenizer(num_words = max_words)\n",
    "    \n",
    "    tokenizer.fit_on_texts(data_x)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # Flatten data_y and create hashing table using set logic\n",
    "    data_y_flattened = [item for sublist in data_y for item in sublist]\n",
    "    data_x_flattened = [item for sublist in data_x for item in sublist]\n",
    "    \n",
    "    labels_index = dict([(y, x + 1) for x, y in enumerate(sorted(set(data_y_flattened)))])\n",
    "    labels = []\n",
    "    for item in data_y:\n",
    "        labels.append([labels_index.get(i) for i in item])\n",
    "    labels_lookup = {v : k for k, v in labels_index.items()} # reverse dictionary for lookup\n",
    "    # save hash tables to disk for model serving\n",
    "    for item, name in zip([word_index, labels_index, labels_lookup],\n",
    "                          [\"word_index\", \"labels_index\", \"labels_lookup\"]):\n",
    "        save_obj(save_dir, item, name)\n",
    "    return word_index, labels_index, labels_lookup\n",
    "\n",
    "word_index, labels_index, labels_lookup = make_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACTOR': 1,\n",
       " 'CHARACTER': 2,\n",
       " 'DIRECTOR': 3,\n",
       " 'GENRE': 4,\n",
       " 'O': 5,\n",
       " 'PLOT': 6,\n",
       " 'RATING': 7,\n",
       " 'RATINGS_AVERAGE': 8,\n",
       " 'REVIEW': 9,\n",
       " 'SONG': 10,\n",
       " 'TITLE': 11,\n",
       " 'TRAILER': 12,\n",
       " 'YEAR': 13}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a generator to serve as input to the tensorflow DataSet API. The `generate_batches` function takes training data in BIO format and yields batches as input to the model function. The DataSet API requires two inputs - features and labels. For a recurrent neural network, we also need to specify sequence lengths to mask variable length sequences. This length is returned as a tuple in the features, as `(batch_x, lengths)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(file = \"data/train.txt\", params = params, train = True):\n",
    "    ''' Generate minibatch with dimensions:\n",
    "    batch_x : (batch_size, max_len)\n",
    "    lengths : (batch_size,)\n",
    "    batch_y : (batch_size, num_classes)\n",
    "    \n",
    "    file : path to .txt containing training data in BIO format\n",
    "    '''\n",
    "    \n",
    "    batch_size = params['batch_size']\n",
    "    max_len = params['padding_size']\n",
    "    save_dir = params['save_dir']\n",
    "    \n",
    "    # load hash tables for tokenization\n",
    "    for item, name in zip([word_index, labels_index, labels_lookup],\n",
    "                          [\"word_index\", \"labels_index\", \"labels_lookup\"]):\n",
    "        item = load_obj(save_dir, name)\n",
    "    \n",
    "    while True:\n",
    "        with open(file, 'r') as f:\n",
    "            batch_x, lengths, batch_y = [], [], []\n",
    "            words, labels = [], []\n",
    "            for line in f:\n",
    "                line = line.strip(\"\\n\").split(\"\\t\")\n",
    "                # lines with len > 1 are words\n",
    "                if len(line) > 1:\n",
    "                    labels.append(line[0][2:]) if len(line[0]) > 1 else labels.append(line[0])\n",
    "                    words.append(line[1])\n",
    "\n",
    "                # lines with len == 1 are breaks between sentences\n",
    "                if len(line) == 1: \n",
    "                    words = [word_index.get(x) if x in word_index.keys() else 0 for x in words]\n",
    "                    labels = [labels_index.get(y) for y in labels]\n",
    "                    batch_x.append(words)\n",
    "                    batch_y.append(labels)\n",
    "                    lengths.append(min(len(words), max_len))\n",
    "                    words, labels = [], []\n",
    "\n",
    "                if len(batch_x) == batch_size:\n",
    "                    batch_x = pad_sequences(batch_x, maxlen = max_len, value = 0, padding = \"post\")\n",
    "                    batch_y = pad_sequences(batch_y, maxlen = max_len, value = 0, padding = \"post\")\n",
    "                    yield (batch_x, lengths), batch_y \n",
    "                    batch_x, lengths, batch_y = [], [], []\n",
    "            if train == False:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimator API requires an input function and a model function. The `input_fn` maps the `generate_batches` generator to a tensorflow Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model training, we need an input function that will feed a tf.Dataset\n",
    "def input_fn(file, params = None, train = True):\n",
    "    params = params if params is not None else {}\n",
    "    shapes = (([None, None], [None]), [None, None]) # batch_x, lengths, batch_y shapes\n",
    "    types = ((tf.int32, tf.int32), tf.int32)        # batch_x, lengths, batch_y data types\n",
    "    \n",
    "    generator = partial(generate_batches, file, train = train)\n",
    "    dataset = tf.data.Dataset.from_generator(generator, types, shapes)\n",
    "    return dataset\n",
    "\n",
    "# For model serving, we need a serving function that will feed tf.placeholders\n",
    "def serving_input_fn():\n",
    "    words = tf.placeholder(dtype=tf.int32, shape=[None, None], name='words')\n",
    "    length = tf.placeholder(dtype=tf.int32, shape=[None], name='length')\n",
    "    receiver_tensors = {'words': words, 'length': length}\n",
    "    features = {'words': words, 'length': length}\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!pip install --user install tensorflow==1.3.0\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model_fn` unpacks features and labels to create the specified model architecture, which is an LSTM-CRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params = params):\n",
    "    # import the data and unpack the features\n",
    "    # serving input_fn returns a dict, convert to multivalue obj\n",
    "    if isinstance(features, dict):\n",
    "        features = features['words'], features['length']\n",
    "    \n",
    "    words, length = features\n",
    "    \n",
    "    # Embedding\n",
    "    embedding = tf.Variable(tf.random.normal([params['max_words'], params['dim']]))\n",
    "    embedding_lookup_for_x = tf.nn.embedding_lookup(embedding, words)\n",
    "    \n",
    "    # LSTM\n",
    "    lstm_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)\n",
    "    lstm_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)\n",
    "    states, final_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                        cell_fw = lstm_cell_fw, \n",
    "                                        cell_bw = lstm_cell_bw,\n",
    "                                        inputs = embedding_lookup_for_x, \n",
    "                                        dtype = tf.float32,\n",
    "                                        time_major = False,\n",
    "                                        sequence_length = length)\n",
    "    lstm_out = tf.concat([states[0], states[1]], axis = 2)\n",
    "        \n",
    "    # Conditional random fields\n",
    "    logits = tf.layers.dense(lstm_out, params['num_classes'])\n",
    "    crf_params = tf.get_variable(\"crf\", [params['num_classes'], params['num_classes']], dtype=tf.float32)\n",
    "    pred_ids, _ = tfa.text.crf_decode(logits, crf_params, length)\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    # Prediction\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = { \n",
    "            'pred_ids': pred_ids,\n",
    "            'tags': words,\n",
    "            'length' : length,\n",
    "        }\n",
    "        export_outputs = {\n",
    "          'prediction': tf.estimator.export.PredictOutput(predictions)\n",
    "      }\n",
    "        \n",
    "    return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\n",
    "    \n",
    "    # Loss functions and optimizers\n",
    "    log_likelihood, _ = tfa.text.crf_log_likelihood(logits, labels, length, crf_params)\n",
    "    \n",
    "    loss = tf.reduce_mean(-log_likelihood)\n",
    "    train_op = tf.train.AdamOptimizer().minimize(\n",
    "        loss, global_step = tf.train.get_or_create_global_step())\n",
    "        \n",
    "    # Training\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode = mode,\n",
    "                                          loss = loss,\n",
    "                                          \n",
    "                                          train_op = train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Estimator API requires separate \"Spec\" objects, through `tf.estimator.TrainSpec` and `EvalSpec` for training and evaluation configuration. We use `functools.partial` to modify the input to the `input_fn` to create separate training and evaluation inputs, and then create separate `Spec` objects for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'models/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Spin up the estimator\n",
    "config = tf.estimator.RunConfig()\n",
    "estimator = tf.estimator.Estimator(model_fn, 'models/model', config, params)\n",
    "\n",
    "# Create train spec\n",
    "train_input_fn = partial(input_fn, \"data/train.txt\", params = params)\n",
    "train_spec = tf.estimator.TrainSpec(train_input_fn)\n",
    "\n",
    "# Create evaluation spec\n",
    "eval_input_fn = partial(input_fn, \"data/test.txt\", params = params, train = False)\n",
    "eval_spec = tf.estimator.EvalSpec(eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into models/model/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 25.085701, step = 1\n",
      "INFO:tensorflow:global_step/sec: 19.995\n",
      "INFO:tensorflow:loss = 5.4256563, step = 101 (5.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.9515\n",
      "INFO:tensorflow:loss = 2.4849465, step = 201 (5.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.3072\n",
      "INFO:tensorflow:loss = 1.2047207, step = 301 (5.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.8533\n",
      "INFO:tensorflow:loss = 4.007087, step = 401 (5.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.4696\n",
      "INFO:tensorflow:loss = 5.614001, step = 501 (4.885 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.0056\n",
      "INFO:tensorflow:loss = 1.0156494, step = 601 (5.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.7823\n",
      "INFO:tensorflow:loss = 0.40496993, step = 701 (5.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.0771\n",
      "INFO:tensorflow:loss = 3.0140467, step = 801 (5.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.4291\n",
      "INFO:tensorflow:loss = 1.768981, step = 901 (4.895 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1000...\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into models/model/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1000...\n",
      "INFO:tensorflow:Loss for final step: 0.42411834.\n",
      "Completed in 55 seconds\n",
      "WARNING:tensorflow:From <ipython-input-46-cda9b03471e7>:5: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function has been renamed, use `export_saved_model` instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /Users/majiga/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:200: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['prediction', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from models/model/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: models/saved_model/temp-1597835460/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'models/saved_model/1597835460'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "estimator.train(input_fn = train_input_fn, max_steps = 1000)\n",
    "te = time.time()\n",
    "print(\"Completed in {} seconds\".format(int(te - ts)))\n",
    "estimator.export_savedmodel('models/saved_model/', serving_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although one might normally use `if mode == tf.estimator.ModeKeys.EVAL` in the `model_fn` to specify evaluation metrics with `tf.metrics`, NER requires multi-class precision, recall, and F1-score which are not available in `tf.metrics`. Instead, we load the true test labels and calculate precision, recall, and F1-score based upon the model predictions for each sentence at the entity-level (discarding non-entity words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/model/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Precision: 0.881 \n",
      "Recall: 0.895 \n",
      "F1-score: 0.878\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predictions = estimator.predict(eval_input_fn)\n",
    "\n",
    "# Load hash tables and true labels\n",
    "labels_index = load_obj(params['save_dir'], \"labels_index\")\n",
    "_, true = load_data(\"data/test.txt\")\n",
    "\n",
    "# Specify which label_index is non-entity\n",
    "dummy_label = labels_index.get(\"O\") \n",
    "\n",
    "# Convert [[string, string], [string, string] ...] to [[int, int], [int, int]]\n",
    "# with hashing table for label indexes\n",
    "labels = []\n",
    "for row in true:\n",
    "    labels.append([labels_index.get(y) for y in row])\n",
    "    \n",
    "# Loop through preds, labels and calculate metrics\n",
    "precisions, recalls, f1s = [], [], []\n",
    "for pred, true in zip(predictions, labels):\n",
    "    pred = pred['pred_ids'][:pred['length']] # undo pad_sequences\n",
    "    pred = [x for x in pred if x != dummy_label] # remove preds that aren't entities\n",
    "    true = np.asarray([x for x in true if x != dummy_label])\n",
    "    recall = calc_recall(true, pred)\n",
    "    recalls.append(recall)\n",
    "    precision = calc_precision(true, pred)\n",
    "    precisions.append(precision)\n",
    "    f1s.append(calc_f1(precision, recall))\n",
    "    \n",
    "print(\"Precision: {} \\nRecall: {} \\nF1-score: {}\".format(np.around(np.mean(precisions), 3),\n",
    "                                                         np.around(np.mean(recalls), 3),\n",
    "                                                         np.around(np.mean(f1s), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving model for on-the-fly predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-8437cccd2a07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLINE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'did george clooney make a science fiction movie in the 1980s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tensorflow.contrib import predictor\n",
    "\n",
    "LINE = 'did george clooney make a science fiction movie in the 1980s'\n",
    "\n",
    "\n",
    "def predict(line, export_dir = 'models/saved_model/', params = params):\n",
    "    # Load hash tables\n",
    "    word_index = load_obj(params['save_dir'], \"word_index\")\n",
    "    labels_lookup = load_obj(params['save_dir'], \"labels_lookup\")\n",
    "   \n",
    "    # Identify and load model weights\n",
    "    subdirs = [x for x in Path(export_dir).iterdir()\n",
    "                   if x.is_dir() and 'temp' not in str(x)]\n",
    "    latest_model = str(sorted(subdirs)[-1])\n",
    "    predict_fn = predictor.from_saved_model(latest_model)\n",
    "               \n",
    "    # Preprocess sentence input\n",
    "    line = line.strip().split()\n",
    "    vector = [word_index.get(x) if x in word_index.keys() else 0 for x in line] # tokenize\n",
    "    vector[len(vector):20] = [0] * (20 - len(vector)) # pad prediction\n",
    "       \n",
    "    # Calculate precision and transform for display\n",
    "    predictions = predict_fn({'words': [vector], 'length': [len(line)]})\n",
    "    tags = predictions.get('tags')\n",
    "    preds = predictions.get('pred_ids')\n",
    "    for tag, pred in zip(tags, preds):\n",
    "        tag = [word for word in tag if word != 0] # unpad\n",
    "        pred = pred[:len(tag)]\n",
    "        pred = [labels_lookup.get(num) for num in pred] #untokenize\n",
    "        print(line, \"\\n\", pred)\n",
    "   \n",
    "predict(LINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.3/kfp-components/notebooks/entity_extraction/assets/fig10.png\" align = \"left\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook shows two end-to-end approaches of training and serving custom NER models including loading and transforming data, creating an NER pipeline, and calculating performance metrics. The custom architecture developed in Tensorflow was competitive with out-of-the-box algorithms while being an order of magnitude faster to train and being able to use the powerful high level APIs in tensorflow like Dataset and Estimator for scalable serving.\n",
    "\n",
    "Both approaches perform well on diverse queries about movies with spelling mistakes and complicated query structures. NER pipelines like the ones presented in this notebook can be integrated into recommender systems, search engines, NLP feature engineering, and customer support / chatbots, among many other business applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
